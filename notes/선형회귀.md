---
title: 선형회귀
tags: [선형대수학, 기계학습]
---

선형회귀(linear regression)이란 하나 이상의 목적변수를 하나 이상의 설명변수로 설명한 식이다.
$w_0, \cdots, w_k$를 가중치, $x_1, \cdots, x_k$를 설명변수라고 할 때 목적변수 $y$를 설명하는 단순 선형 회귀식은 아래와 같다:

$$
y = w_0 + \sum^l_{k=1}{w_kx_k}
$$

여러 목적 변수를 설명하는 다중 선형 회귀는 행렬을 이용해 표기하며 식은 아래와 같다:

- $\mathbf y$: 설명변수가 담긴 열벡터
- $X$: 확률변수가 담긴 행렬
- $\mathbf w$: 가중치가 담긴 열벡터
- $\mathbf \varepsilon$: 오차항 열벡터

$$
\mathbf{y} = X\mathbf{w} + \mathbf \varepsilon
$$

## 가중치 구하기

데이터를 잘 설명하는 모델을 만들기 위해서는 가중치 $\mathbf w$를 잘 구해야 한다.

- 유사역행렬
- 경사하강법(gradient descent)
	- 
- 확률적 경사하강법(SGD; stochastic gradient descent)
	- 볼록하지 않은(non-convex) 목적식을 최적화할 수 있다.
- 최소제곱법(least squared method)
	- 평균제곱오차(MSE)를 각각의 가중치에 대해 편미분한 값이 0이 되도록 연립방정식을 풀어 가중치를 구한다.

### 최소제곱법

아래와 같은 행렬식에서 적절한 가중치 $\mathbf w$를 구해서 선형회귀를 완성하려고 한다. 이때 $\mathbf y$의 각 원소 $y_i$는 아래와 같이 풀어진다.

$$
\begin{align}
\mathbf y &= X \mathbf w + \mathbf \varepsilon \\
y_i &= w_1x_{i1} + \cdots + w_mx_{im} + \varepsilon_i
\end{align}
$$

원래 데이터의 값을 $f(x_{i1}, \cdots, x_{im})$이라고 했을 때, 오차의 제곱합 $D$는 아래와 같다.

$$
D=\sum^n_{i=1}\{f(x_{i1},\cdots,x_{im})-y_i\}^2
$$

TODO